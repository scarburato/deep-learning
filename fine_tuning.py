# -*- coding: utf-8 -*-
"""fine_tuning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1o0IEBjNgJlLE6j93eZKgojF2H9dhGAzF

## Feature extraction
"""

LOCAL_MODE = True

# if not LOCAL_MODE:
#   from google.colab import drive
#   from google.colab.patches import cv2_imshow
#
#   drive.mount('/content/drive', force_remount=True)
#   !cd /content; rm -r processed; 7z x drive/Shareddrives/deep_learning/processed.128_65.7z; ls -alF processed
#
# !pip install tensorflow --quiet


from common import *


"""Let's use the convolutional base of the VGG16 network, trained on ImageNet, to extract interesting features from
our audio images.
"""

conv_base = tf.keras.applications.vgg16.VGG16(
    input_shape=(HEIGHT,WIDTH,3),
    weights="imagenet",
    include_top=False,
)

conv_base.summary()

"""## Layers freezing
Before we compile and train our model, a very important thing to do is to freeze the convolutional base. "Freezing" a layer or set of
layers means preventing their weights from getting updated during training. If we don't do this, then the representations that were
previously learned by the convolutional base would get modified during training. Since the classifier on top (i.e., the `Dense` layers we will add) is randomly initialized,
very large weight updates would be propagated through the network, effectively destroying the representations previously learned.

In Keras, freezing a network is done by setting its `trainable` attribute to `False`:
"""

import numpy as np

print('This is the number of trainable weights '
      'before freezing the conv base:', sum(np.prod(x.shape) for x in conv_base.trainable_weights))

conv_base.trainable = False

print('This is the number of trainable weights '
      'after freezing the conv base:', sum(np.prod(x.shape) for x in conv_base.trainable_weights))

"""Now we can create a new model that chains together
1. A data augmentation stage
2. Our frozen convolutional base
3. A dense classifier
"""

class Gray2VGGInput(tf.keras.layers.Layer):
    """
    Custom conversion layer
    """
    def build( self, x ) :
        self.image_mean = tf.keras.backend.variable(
            value=np.array([103.939, 116.779, 123.68])
              .reshape([1,1,1,3]).astype('float32'), dtype='float32', name='imageNet_mean' )
        self.built = True
        return

    def call( self, x ) :

        rgb_x = tf.keras.backend.concatenate( [x,x,x], axis=-1 )
        norm_x = rgb_x - self.image_mean
        return norm_x

    def compute_output_shape( self, input_shape ) :
        return input_shape[:3] + (3,)


model = tf.keras.Sequential([
    tf.keras.Input(shape=(HEIGHT, WIDTH, COLOR_MODE.channels), name="ingresso-monocromo"),
    # Add remove this if colormode is rgb
    Gray2VGGInput(name = "convertitore-a-colori"),
    tf.keras.layers.Rescaling(1./255),
    conv_base,
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dropout(0.66),
    tf.keras.layers.Dense(N_CLASSES, activation='softmax'),
], name="layer-freeze-VGG16")

compile_model(model)


model.summary()

evalutate(model, "vgg16_feature_extract")

"""# Fine tuning

"""

conv_base.summary()

conv_base.trainable = True

set_trainable = False
for layer in conv_base.layers:
    if layer.name == 'block5_conv1':
        set_trainable = True
    if set_trainable:
        layer.trainable = True
    else:
        layer.trainable = False

conv_base.summary()

compile_model(model)

model.summary()

evalutate(model, "vgg16_finetune")