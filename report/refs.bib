@inproceedings{krautdb,
	author = {Burkhardt, Felix and Paeschke, Astrid and Rolfes, M. and Sendlmeier, Walter and Weiss, Benjamin},
	year = {2005},
	month = {09},
	pages = {1517-1520},
	title = {A database of German emotional speech},
	volume = {5},
	journal = {9th European Conference on Speech Communication and Technology},
	doi = {10.21437/Interspeech.2005-446}
}

@inproceedings{costantini-etal-2014-emovo,
	title = "{EMOVO} Corpus: an {I}talian Emotional Speech Database",
	author = "Costantini, Giovanni  and
	Iaderola, Iacopo  and
	Paoloni, Andrea  and
	Todisco, Massimiliano",
	booktitle = "Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14)",
	month = may,
	year = "2014",
	address = "Reykjavik, Iceland",
	publisher = "European Language Resources Association (ELRA)",
	url = "http://www.lrec-conf.org/proceedings/lrec2014/pdf/591_Paper.pdf",
	pages = "3501--3504",
	abstract = "This article describes the first emotional corpus, named EMOVO, applicable to Italian language,. It is a database built from the voices of up to 6 actors who played 14 sentences simulating 6 emotional states (disgust, fear, anger, joy, surprise, sadness) plus the neutral state. These emotions are the well-known Big Six found in most of the literature related to emotional speech. The recordings were made with professional equipment in the Fondazione Ugo Bordoni laboratories. The paper also describes a subjective validation test of the corpus, based on emotion-discrimination of two sentences carried out by two different groups of 24 listeners. The test was successful because it yielded an overall recognition accuracy of 80{\%}. It is observed that emotions less easy to recognize are joy and disgust, whereas the most easy to detect are anger, sadness and the neutral state.",
}

@article{adigwe2018emotional,
	title={The emotional voices database: Towards controlling the emotion dimension in voice generation systems},
	author={Adigwe, Adaeze and Tits, No{\'e} and Haddad, Kevin El and Ostadabbas, Sarah and Dutoit, Thierry},
	journal={arXiv preprint arXiv:1806.09514},
	year={2018}
}

@article{jl-corpus,
	author = {James, Jesin and Tian, Li and Watson, Catherine},
	year = {2018},
	month = {09},
	pages = {2768-2772},
	title = {An Open Source Emotional Speech Corpus for Human Robot Interaction Applications},
	doi = {10.21437/Interspeech.2018-1349}
}

@misc{poria2019meld,
	title={MELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations}, 
	author={Soujanya Poria and Devamanyu Hazarika and Navonil Majumder and Gautam Naik and Erik Cambria and Rada Mihalcea},
	year={2019},
	eprint={1810.02508},
	archivePrefix={arXiv},
	primaryClass={cs.CL}
}

@dataset{livingstone_steven_r_2018_1188976,
	author       = {Livingstone, Steven R. and
	Russo, Frank A.},
	title        = {{The Ryerson Audio-Visual Database of Emotional 
	Speech and Song (RAVDESS)}},
	month        = apr,
	year         = 2018,
	note         = {{Funding Information Natural Sciences and 
	Engineering Research Council of Canada:
	2012-341583  Hear the world research chair in
	music and emotional speech from Phonak}},
	publisher    = {Zenodo},
	version      = {1.0.0},
	doi          = {10.5281/zenodo.1188976},
	url          = {https://doi.org/10.5281/zenodo.1188976}
}

@online{womansarticle,
	author = {Muriel Kosaka},
	title = {VGG-16 Transfer Learning in Classifying Log-Mel Spectrogram Images},
	year = 2020,
	url = {https://towardsdatascience.com/transfer-learning-in-speech-emotion-recognition-d55b6616ba83},
	urldate = {\date{}}
}

@misc{enwiki:1173975697,
	author = "{Wikipedia contributors}",
	title = "Occam's razor --- {Wikipedia}{,} The Free Encyclopedia",
	year = "2023",
	howpublished = "\url{https://en.wikipedia.org/w/index.php?title=Occam%27s_razor&oldid=1173975697}",
	note = "[Online; accessed 15-September-2023]"
}
