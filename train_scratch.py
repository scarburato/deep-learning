# -*- coding: utf-8 -*-
"""train_scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/149X0IBt7SwsCtlznok8OAKAqr8GQU9Cz

# Train-Scratch

In this file we TRAIN.
"""

LOCAL_MODE = True

#if not LOCAL_MODE:
#  from google.colab import drive
#  from google.colab.patches import cv2_imshow
#
#  drive.mount('/content/drive', force_remount=True)
#
#  !cd /content; rm -r processed; 7z x drive/Shareddrives/deep_learning/processed.128_87.7z; ls -alF processed
#  TRAIN_PATH = "/content/processed/"
#  LOCAL_MODELS_FOLDER = "/content/drive/Shareddrives/deep_learning/models"
#
#!pip install tensorflow --quiet

import numpy as np
import os
import tensorflow as tf
import matplotlib.pyplot as plt
import sys
import shutil
import itertools
from sklearn import metrics



if LOCAL_MODE:
  TRAIN_PATH = sys.argv[1]
  MODELS_FOLDER = sys.argv[2]

  assert os.path.isdir(TRAIN_PATH), "arg 1 must be a folder!"

  os.makedirs(MODELS_FOLDER, exist_ok=True)
  assert os.path.isdir(MODELS_FOLDER), "arg 2 must be a folder!"

#!mkdir -p /content/drive/Shareddrives/deep_learning/models

"""# ADDESTRAMENTO

## No augmentation
"""

BATCH_SIZE = 64
WIDTH, HEIGHT = 128, 65
EPOCHS = 50

# helper class to switch between color-modes
class Colors:
    class ColorMode:
        def __init__(self, keyword:str, channels:int):
            self.keyword = keyword
            self.channels = channels

    # Define color modes as class instances
    RGB = ColorMode('rgb', 3)
    GRAYSCALE = ColorMode('grayscale', 1)


COLOR_MODE = Colors.GRAYSCALE

# compile_model compiles a model
def compile_model(model):

  import tensorflow_addons as tfa

  f1 = tfa.metrics.F1Score(num_classes=N_CLASSES)

  model.compile(
      optimizer='adam',
      loss='categorical_crossentropy',
      metrics=[
        f1,
        "accuracy"
      ],
  )

def get_model_folder(name : str):
    return os.path.join(MODELS_FOLDER, name)

def get_model_weights_path(name : str):
   return  os.path.join(get_model_folder(name), "model.keras")

def plot(model : any, history : any, name : str):

  out_folder = get_model_folder(name)
  # Plot training history f1_score
  plt.figure(figsize=(10,10))
  plt.plot(np.mean(history.history['f1_score'], axis=1), label='f1_score')
  plt.plot(np.mean(history.history['val_f1_score'], axis=1), label='val_f1_score')
  plt.xlabel('Epoch')
  plt.ylabel('F1-score')
  plt.ylim([0, 1])
  plt.legend(loc='lower right')
  plt.show()
  plt.savefig(os.path.join(out_folder, "learning_history-f1_score.png"))
  plt.close()


  # Plot training history loss
  plt.figure(figsize=(10,10))
  plt.plot(history.history['loss'], label='loss')
  plt.plot(history.history['val_loss'], label='val_loss')
  plt.xlabel('Epoch')
  plt.ylabel('Loss')
  plt.ylim([0, 1])
  plt.legend(loc='lower right')
  plt.show()
  plt.savefig(os.path.join(out_folder, "learning_history-loss.png"))
  plt.close()

  # Plot training history accuracy
  plt.figure(figsize=(10,10))
  plt.plot(history.history['accuracy'], label='accuracy')
  plt.plot(history.history['val_accuracy'], label='val_accuracy')
  plt.xlabel('Epoch')
  plt.ylabel('Accuracy')
  plt.ylim([0, 1])
  plt.legend(loc='lower right')
  plt.show()
  plt.savefig(os.path.join(out_folder, "learning_history-acc.png"))
  plt.close()

  # PLot confusion matrix
  preds = model.predict(X_test)
  Y_pred = np.argmax(preds, axis=1)

  cm = metrics.confusion_matrix(Y_test, Y_pred, normalize = 'true')
  cm = np.trunc(cm*10**2)/(10**2)

  # LOG this should be equal to the original one
#   correct_predictions = sum(1 for p, t in zip(Y_pred, Y_test) if p == t)
#   total_predictions = len(Y_pred)
#   accuracy = correct_predictions / total_predictions
#   print("Accuracy: ", accuracy)
  # LOG

  plt.figure(figsize=(10, 10))
  plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
  plt.title('Confusion Matrix')
  plt.colorbar()

  tick_marks = np.arange(N_CLASSES)
  plt.xticks(tick_marks, CLASSES, rotation=45)
  plt.yticks(tick_marks, CLASSES)

  thresh = cm.max() / 2.
  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
    plt.text(j, i, cm[i, j], horizontalalignment='center', color='white' if cm[i, j] > thresh else 'black')

  plt.tight_layout()
  plt.ylabel('True Label')
  plt.xlabel('Predicted Label')

  plt.show()
  plt.savefig(os.path.join(out_folder, "confusion_matrix.png"))
  plt.close()

# train trains a model and put its weight in the specified output path
def train(model : any, name : str):
    
    # define useful callbacks
    early_stop = tf.keras.callbacks.EarlyStopping(
        monitor='loss',
        min_delta=0.02,
        patience=6,
    )

    save_best_model = tf.keras.callbacks.ModelCheckpoint(
        filepath=get_model_weights_path(name),
        monitor='val_loss',
        save_best_only=True,
        save_weights_only=True,
    )

    csv_logger = tf.keras.callbacks.CSVLogger(
      os.path.join(get_model_folder(name), "model.stats.csv"),
      append=True
    )

    # Train the model
    history = model.fit(
        train_dataset,
        epochs=EPOCHS,
        validation_data=val_dataset,
        validation_steps=len(val_dataset),
        callbacks=[
            csv_logger,
            early_stop,
            save_best_model
        ],

        verbose=1,
        workers=4
    )

    return history

  
def evalutate(model : any, name : str):
    model_out_folder = get_model_folder(name)

    if os.path.exists(model_out_folder):
        shutil.rmtree(model_out_folder)

    os.makedirs(model_out_folder, exist_ok=True)

    history = train(model, name)

    plot(model, history, name)
   
"""La roba vera per l'addestrament insomma"""

from tensorflow.keras.utils import image_dataset_from_directory

# Load the dataset without validation splitting
dataset = image_dataset_from_directory(
    TRAIN_PATH,
    image_size=(HEIGHT, WIDTH),
    color_mode=COLOR_MODE.keyword,
    batch_size=BATCH_SIZE,
    label_mode="categorical",
    shuffle=True,
    seed=42
)


"""devide the dataset and log info"""


CLASSES = dataset.class_names
N_CLASSES = len(dataset.class_names)

# Calculate the number of validation samples
N_SAMPLES = dataset.cardinality().numpy()

VALIDATION_TEST_SAMPLES = int(0.3 * N_SAMPLES)  # 30% of data for validation and test
VALIDATION_SAMPLES = int(0.6 * VALIDATION_TEST_SAMPLES)

# Split the dataset into training and validation
val_test_dataset = dataset.take(VALIDATION_TEST_SAMPLES)
train_dataset = dataset.skip(VALIDATION_TEST_SAMPLES)

test_dataset = val_test_dataset.take(VALIDATION_SAMPLES)
val_dataset = val_test_dataset.skip(VALIDATION_SAMPLES)


X_test = []
Y_test = []

for images, labels in test_dataset:
    for image in images:
        X_test.append(np.array(image.numpy().tolist()))  # append list
    for label in labels:
        Y_test.append(np.argmax(label.numpy(), axis=0)) 

X_test = tf.constant(X_test)


model = tf.keras.Sequential([
    tf.keras.Input(shape=(HEIGHT, WIDTH, COLOR_MODE.channels)),
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(256, activation='relu'),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.3),
    tf.keras.layers.Dense(N_CLASSES, activation='softmax')
])

compile_model(model)
model.summary()

evalutate(model, "model0")

"""## Augmentation


"""

data_augmentation = tf.keras.Sequential([
  tf.keras.layers.RandomFlip("horizontal"), # Applies horizontal flipping to a random 50% of the images
  tf.keras.layers.RandomRotation(0.1), # Rotates the input images by a random value in the range[–10%, +10%] (fraction of full circle [-36°, 36°])
  tf.keras.layers.RandomZoom(0.2), # Zooms in or out of the image by a random factor in the range [-20%, +20%]
], name="ruotaingrandimento")

model = tf.keras.Sequential([
    tf.keras.Input(shape=(HEIGHT, WIDTH, COLOR_MODE.channels)),
    tf.keras.layers.Rescaling(1./255),
    data_augmentation,
    tf.keras.layers.RandomContrast(0.5),
    tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation="relu",),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Conv2D(filters=64, kernel_size=3, activation="relu"),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Conv2D(filters=128, kernel_size=3, activation="relu"),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Conv2D(filters=256, kernel_size=3, activation="relu"),
    tf.keras.layers.MaxPooling2D(pool_size=2),

    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(N_CLASSES, activation='softmax')
])

compile_model(model)
model.summary()
evalutate(model, "model1")

# HJ
model = tf.keras.Sequential([
    tf.keras.Input(shape=(HEIGHT, WIDTH, COLOR_MODE.channels)),
    tf.keras.layers.Rescaling(1./255),
    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(256, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Conv2D(512, (3, 3), activation='relu'),
    tf.keras.layers.MaxPooling2D((2, 2)),
    tf.keras.layers.Flatten(),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(1024, activation='relu'),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dropout(0.6),
    tf.keras.layers.Dense(N_CLASSES, activation='softmax')
])

compile_model(model)
model.summary()
evalutate(model, "model2")
